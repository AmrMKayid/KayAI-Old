{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "frozen_lake_q_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "6lRGl5WXyiX-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Q-Learning Frozen Lake"
      ]
    },
    {
      "metadata": {
        "id": "psziltQSp0-s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DILn-tdStlri",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating the environment"
      ]
    },
    {
      "metadata": {
        "id": "sAiP0QnUrEBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "58b2ff4c-ad54-431a-b8ec-8a7ed784f269"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "kBG69YAQtpzK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating the Q-table"
      ]
    },
    {
      "metadata": {
        "id": "SD4sxnxUrV5I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "\n",
        "q_table = np.zeros((state_space_size, action_space_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0VdViP7zr9b5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "b7367b1a-e0de-4727-ea11-de69d5b5a25b"
      },
      "cell_type": "code",
      "source": [
        "q_table"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "wkUDxvZpttZN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initializing Q-learning parameters"
      ]
    },
    {
      "metadata": {
        "id": "hXqf1J8BsCzX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_episodes = 10000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "learning_rate = 0.1            # Alpha\n",
        "discounting_rate = 0.99        # Gamma \n",
        "\n",
        "# Exploration & Exploitation Trade-off\n",
        "exploration_rate = 1           # Epsilon \n",
        "max_exploration_rate = 1\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VWTfO1C_zU1b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Rewards"
      ]
    },
    {
      "metadata": {
        "id": "WF2M8EkBzaIe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rewards_all_episodes = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O8j_-kQTzdkw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q-Learning Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "qfZB7qT7zijb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    \n",
        "    done = False\n",
        "    rewards_current_episode = 0\n",
        "    \n",
        "    for step in range(max_steps_per_episode):\n",
        "        \n",
        "        # Exploration-Eploitation trade-off\n",
        "        exploration_rate_threshold = random.uniform(0, 1)\n",
        "        if exploration_rate_threshold > exploration_rate:\n",
        "            action = np.argmax(q_table[state, :])\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "            \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        # Update Q-table for Q(s, a)\n",
        "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
        "                                 learning_rate * (reward + discounting_rate * np.max(q_table[new_state, :]))\n",
        "        \n",
        "        state = new_state\n",
        "        rewards_current_episode += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "                                                  \n",
        "        # Exploration rate decay\n",
        "        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
        "                                                  \n",
        "        rewards_all_episodes.append(rewards_current_episode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T5BnDON-12ta",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Results"
      ]
    },
    {
      "metadata": {
        "id": "ev445FQf2PQt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "42707273-4437-4830-d75c-3807bd7b5689"
      },
      "cell_type": "code",
      "source": [
        "q_table"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.56397331, 0.51569156, 0.52688727, 0.50889841],\n",
              "       [0.31637959, 0.32711117, 0.15751568, 0.50688639],\n",
              "       [0.41890206, 0.39984262, 0.3864757 , 0.46504202],\n",
              "       [0.35394541, 0.32622205, 0.25307766, 0.44604215],\n",
              "       [0.58628871, 0.51754179, 0.35072974, 0.3272768 ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.17399197, 0.11566206, 0.25851104, 0.0899934 ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.35950817, 0.46682029, 0.27996905, 0.61784997],\n",
              "       [0.46363685, 0.67028635, 0.37827683, 0.37184505],\n",
              "       [0.66313902, 0.45485946, 0.42534938, 0.33671569],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.43884575, 0.65323892, 0.75951123, 0.51006752],\n",
              "       [0.76916141, 0.8872896 , 0.75401537, 0.74459843],\n",
              "       [0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "_RRbNn3G3De8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Watch Q-learning agent play Frozen Lake"
      ]
    },
    {
      "metadata": {
        "id": "4-CGqtGh3mTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "2033177e-c1db-40f7-8917-f44d6a787d90"
      },
      "cell_type": "code",
      "source": [
        "for episode in range(3):\n",
        "    # initialize new episode params\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
        "    time.sleep(1)\n",
        "\n",
        "    for step in range(max_steps_per_episode):        \n",
        "        # Show current state of environment on screen\n",
        "        # Choose action with highest Q-value for current state       \n",
        "        # Take new action \n",
        "        clear_output(wait=True)\n",
        "        env.render()\n",
        "        time.sleep(0.3)\n",
        "        \n",
        "        action = np.argmax(q_table[state,:])        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        if done:\n",
        "            clear_output(wait=True)\n",
        "            env.render()\n",
        "            if reward == 1:\n",
        "                # Agent reached the goal and won episode\n",
        "                print(\"****You reached the goal!****\")\n",
        "                time.sleep(3)\n",
        "            else:\n",
        "                # Agent stepped in a hole and lost episode  \n",
        "                print(\"****You fell through a hole!****\")\n",
        "                time.sleep(3)\n",
        "                clear_output(wait=True)\n",
        "            break\n",
        "            \n",
        "        # Set new state\n",
        "        state = new_state\n",
        "        \n",
        "env.close()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "****You reached the goal!****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WpavOk_24Je3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}