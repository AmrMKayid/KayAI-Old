{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8.1-text-generation-with-lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cL27bfsuLiy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c031e47a-3916-454d-a41a-be808d826d74"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "QBj_DqAvLizI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text generation with LSTM\n",
        "\n",
        "This notebook contains the code samples found in Chapter 8, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "[...]\n",
        "\n",
        "## Implementing character-level LSTM text generation\n",
        "\n",
        "\n",
        "Let's put these ideas in practice in a Keras implementation. The first thing we need is a lot of text data that we can use to learn a \n",
        "language model. You could use any sufficiently large text file or set of text files -- Wikipedia, the Lord of the Rings, etc. In this \n",
        "example we will use some of the writings of Nietzsche, the late-19th century German philosopher (translated to English). The language model \n",
        "we will learn will thus be specifically a model of Nietzsche's writing style and topics of choice, rather than a more generic model of the \n",
        "English language."
      ]
    },
    {
      "metadata": {
        "id": "OfvXAdQILizT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the data\n",
        "\n",
        "Let's start by downloading the corpus and converting it to lowercase:"
      ]
    },
    {
      "metadata": {
        "id": "d4s0aEAyLizW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f4246594-e933-4b5d-cef3-225bbeae6c9d"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "path = keras.utils.get_file(\n",
        "    'nietzsche.txt',\n",
        "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "text = open(path).read().lower()\n",
        "print('Corpus length:', len(text))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 0s 0us/step\n",
            "Corpus length: 600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zwc_6Q0qLizd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Next, we will extract partially-overlapping sequences of length `maxlen`, one-hot encode them and pack them in a 3D Numpy array `x` of \n",
        "shape `(sequences, maxlen, unique_characters)`. Simultaneously, we prepare a array `y` containing the corresponding targets: the one-hot \n",
        "encoded characters that come right after each extracted sequence."
      ]
    },
    {
      "metadata": {
        "id": "-_u4p1kgLizf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "6dd8e69f-b25a-416d-9a5f-2a5ede1f03cc"
      },
      "cell_type": "code",
      "source": [
        "# Length of extracted character sequences\n",
        "maxlen = 60\n",
        "\n",
        "# We sample a new sequence every `step` characters\n",
        "step = 3\n",
        "\n",
        "# This holds our extracted sequences\n",
        "sentences = []\n",
        "\n",
        "# This holds the targets (the follow-up characters)\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('Number of sequences:', len(sentences))\n",
        "\n",
        "# List of unique characters in the corpus\n",
        "chars = sorted(list(set(text)))\n",
        "print('Unique characters:', len(chars))\n",
        "# Dictionary mapping unique characters to their index in `chars`\n",
        "char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "\n",
        "# Next, one-hot encode the characters into binary arrays.\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sequences: 200278\n",
            "Unique characters: 57\n",
            "Vectorization...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DiseHlIvLizz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building the network\n",
        "\n",
        "Our network is a single `LSTM` layer followed by a `Dense` classifier and softmax over all possible characters. But let us note that \n",
        "recurrent neural networks are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in \n",
        "recent times."
      ]
    },
    {
      "metadata": {
        "id": "ubzgrMvcLiz3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "afd59f4d-d5da-475c-995d-af96b962d4eb"
      },
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation='softmax'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cL0f88J4Li0M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since our targets are one-hot encoded, we will use `categorical_crossentropy` as the loss to train the model:"
      ]
    },
    {
      "metadata": {
        "id": "--yi8fAKLi0O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RW01k_v5Li0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the language model and sampling from it\n",
        "\n",
        "\n",
        "Given a trained model and a seed text snippet, we generate new text by repeatedly:\n",
        "\n",
        "* 1) Drawing from the model a probability distribution over the next character given the text available so far\n",
        "* 2) Reweighting the distribution to a certain \"temperature\"\n",
        "* 3) Sampling the next character at random according to the reweighted distribution\n",
        "* 4) Adding the new character at the end of the available text\n",
        "\n",
        "This is the code we use to reweight the original probability distribution coming out of the model, \n",
        "and draw a character index from it (the \"sampling function\"):"
      ]
    },
    {
      "metadata": {
        "id": "VVzVjSDxLi0x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmCBY55WLi08",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Finally, this is the loop where we repeatedly train and generated text. We start generating text using a range of different temperatures \n",
        "after every epoch. This allows us to see how the generated text evolves as the model starts converging, as well as the impact of \n",
        "temperature in the sampling strategy."
      ]
    },
    {
      "metadata": {
        "id": "lMv9drkTLi1D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2517
        },
        "outputId": "d413b70c-267f-4606-b151-0f51a7980d75"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import sys\n",
        "\n",
        "for epoch in range(1, 60):\n",
        "    print('epoch', epoch)\n",
        "    # Fit the model for 1 epoch on the available training data\n",
        "    model.fit(x, y,\n",
        "              batch_size=128,\n",
        "              epochs=1)\n",
        "\n",
        "    # Select a text seed at random\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated_text = text[start_index: start_index + maxlen]\n",
        "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
        "\n",
        "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('------ temperature:', temperature)\n",
        "        sys.stdout.write(generated_text)\n",
        "\n",
        "        # We generate 400 characters\n",
        "        for i in range(400):\n",
        "            sampled = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(generated_text):\n",
        "                sampled[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(sampled, verbose=0)[0]\n",
        "            next_index = sample(preds, temperature)\n",
        "            next_char = chars[next_index]\n",
        "\n",
        "            generated_text += next_char\n",
        "            generated_text = generated_text[1:]\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "200278/200278 [==============================] - 190s 948us/step - loss: 2.0267\n",
            "--- Generating with seed: \" to relax, of course, with consideration, and\n",
            "naturally with\"\n",
            "------ temperature: 0.2\n",
            " to relax, of course, with consideration, and\n",
            "naturally with the self-all the self-and the man of the self-and the proficity of the proficise and all the former the forlitions of the self and and all the from the forlitions of the self the self-all the former of the self the provident and the from the profician of the self the from the from the self-all the self-and and all the former the same and all the self of the self-and the self the provided the prea\n",
            "------ temperature: 0.5\n",
            " all the self of the self-and the self the provided the preasumest to and food the will of the histinge that the superit the frare spirition and all the mantering the such sonlition of the sould the are the onations to all this provises, the onother for the olding of the infersting the compulse is all the mansient and eloations and all this abold his and the world and all to will the seright and may in the felle their belief the condition and quality of fr\n",
            "------ temperature: 1.0\n",
            "ay in the felle their belief the condition and quality of frontain, worth\n",
            "distand-doviling the\n",
            "strnece his faons in\n",
            "\n",
            "in asistest, asmusporeing, becaurtamens--arigeal as mistrule than is elnent his arexyeb and\n",
            "as in: thost to dange--which of tagen, whathiegs, probainstor so wholices hit his is, sufats sumes be\n",
            "of a real not mootaniwlity and socerouse thisl but towerner and amaring in\n",
            "onlents, and evel did this net in worlhand not vial dead i alint teximater\n",
            "------ temperature: 1.2\n",
            "vel did this net in worlhand not vial dead i alint teximatery rugtarle soubst had givelegite and dalificatution and anstobutto, oh in still powberom their be viciednterour thing feliffect wolls qureisuration, thus thingshuites, theigher, bestersucifian--\"outringineskers to we hist? awligitials! promiselall attersents, of the hied, and heally! to love dusten,\" that prosuch in low the hust howthis of selfsopwer to even\n",
            "the migrtaminatiog them gor of pains. t\n",
            "epoch 2\n",
            "Epoch 1/1\n",
            "200278/200278 [==============================] - 190s 948us/step - loss: 1.6505\n",
            "--- Generating with seed: \"ample society, the state, subjects the\n",
            "single personalities,\"\n",
            "------ temperature: 0.2\n",
            "ample society, the state, subjects the\n",
            "single personalities, and the ready the case in the same the same the discitute the sense of the exception, and the call the fact of the consequent of the cally of the same and the consequent the contradice to the consible and the same the stand the call the contement and the same and the consider the can in the call the consider and the can in the dutaked the present in the same and the same and the same and the call\n",
            "------ temperature: 0.5\n",
            "e present in the same and the same and the same and the call the love in the contemen to a have mechant it is the religious and indeed the believe to the will the condition, and a specie, and science and man is the instilct the are excection and controly, and the curches and have the exception of the distent and nature of the should and one of the same the caining there of the prepations in the consequent the been and in the feeling and now the religious w\n",
            "------ temperature: 1.0\n",
            "nsequent the been and in the feeling and now the religious was be\n",
            "theer the cimfened, and in mefedity--and what the\n",
            "intleoryly. them\n",
            "selpy the prede, of the repryfound,\n",
            "mackly nourly\n",
            "views\n",
            "take scent--but of exoplate more of quiting hand of a man\n",
            "leth intercourous to see acle of\n",
            "the begoneses, into securess a speciciunity the funitugrain, the tertions toward in\n",
            "the blate) to the reworars to vor longing theed out like the still our contrast--as, social\n",
            "alme\n",
            "------ temperature: 1.2\n",
            "nging theed out like the still our contrast--as, social\n",
            "almently have\n",
            "conside above been the mastim and thereficy, \"ersurn\n",
            "amefy--it is all\n",
            "bret loak, me\n",
            "malyinened--preceive the relicionsch, and cener by seage. one course bquable!y\n",
            "phret to the breadnerly fror\n",
            "doebjecting, syciautiss,\n",
            "however makes itser reart fol thoughancy, the into\n",
            "the cas be folly,\n",
            "by the foucht, which, so hindstory. whoicard\"\n",
            "restan of higherdocties, men shold manyin a matter from of\n",
            "epoch 3\n",
            "Epoch 1/1\n",
            "200278/200278 [==============================] - 191s 951us/step - loss: 1.5599\n",
            "--- Generating with seed: \"nd before man as he now, hardened by the discipline\n",
            "of scien\"\n",
            "------ temperature: 0.2\n",
            "nd before man as he now, hardened by the discipline\n",
            "of science of the interestion of the constitution of the world to the struggel that the constantly and the one of the strength and the sense of the world of the world and the strength to the world to the constitution and the world as the sure a soul the same the strength and the come the problem of the expressions of the presence of the will the consequence of the world to the sure the subless of the prec\n",
            "------ temperature: 0.5\n",
            "consequence of the world to the sure the subless of the preciate the fact on that he precistation of the world--and he strength and for the have been the converes of the act of life, as the constitate and the man as a \"nature of the leds of the constranting the convered may be a that charm of the expression to spirity of the presences which the age of the still as into strength, as it were was be lives the life, the sciently, as a poteem, as a confutions o\n",
            "------ temperature: 1.0\n",
            "lives the life, the sciently, as a poteem, as a confutions ories\n",
            "me has beure more plowass ame here something are eeverness to be do bace\n",
            "does..\n",
            "\n",
            "2ge science, wertord us, so exeme, what hidden of the exery of their enjoyed the\n",
            "sexises astre? revered to be for the was\n",
            "but the ringr of\n",
            "ligtor, \"thus whice grateness of which\n",
            "would be a path of chesenting concerses satisfrended, thinker ideal in the personal cause but here whateveris, it years by persing, in t\n",
            "------ temperature: 1.2\n",
            "ersonal cause but here whateveris, it years by persing, in tootac. a\n",
            "sypeet them thoughtly creas\n",
            "exest, point act by\n",
            "possible pesspalte enever the comes philosophed afleds acted calvole\n",
            "of fecuss,\" youth, so chootwwerming that thonem appell distrapory of exierst, always in nhe calvee on, the general\n",
            "sectrulm of\n",
            "fagrly pays\n",
            "there, however\"?\n",
            "youthentions moralomy head: \"be\n",
            "north that any lacutes--pain, the cas--well, oness, at thoires\n",
            "vulubed at his\n",
            "eurotesi\n",
            "epoch 4\n",
            "Epoch 1/1\n",
            " 23040/200278 [==>...........................] - ETA: 2:49 - loss: 1.5146"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9486ed95c0ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     model.fit(x, y,\n\u001b[1;32m      8\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m               epochs=1)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Select a text seed at random\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "0_rywFNQLi1R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "As you can see, a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in \n",
        "particular, all words (a word being a local pattern of characters) are real English words. With higher temperatures, the generated text \n",
        "becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible (such as \n",
        "\"eterned\" or \"troveration\"). With a high temperature, the local structure starts breaking down and most words look like semi-random strings \n",
        "of characters. Without a doubt, here 0.5 is the most interesting temperature for text generation in this specific setup. Always experiment \n",
        "with multiple sampling strategies! A clever balance between learned structure and randomness is what makes generation interesting.\n",
        "\n",
        "Note that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and \n",
        "realistic than ours. But of course, don't expect to ever generate any meaningful text, other than by random chance: all we are doing is \n",
        "sampling data from a statistical model of which characters come after which characters. Language is a communication channel, and there is \n",
        "a distinction between what communications are about, and the statistical structure of the messages in which communications are encoded. To \n",
        "evidence this distinction, here is a thought experiment: what if human language did a better job at compressing communications, much like \n",
        "our computers do with most of our digital communications? Then language would be no less meaningful, yet it would lack any intrinsic \n",
        "statistical structure, thus making it impossible to learn a language model like we just did.\n",
        "\n",
        "\n",
        "## Take aways\n",
        "\n",
        "* We can generate discrete sequence data by training a model to predict the next tokens(s) given previous tokens.\n",
        "* In the case of text, such a model is called a \"language model\" and could be based on either words or characters.\n",
        "* Sampling the next token requires balance between adhering to what the model judges likely, and introducing randomness.\n",
        "* One way to handle this is the notion of _softmax temperature_. Always experiment with different temperatures to find the \"right\" one."
      ]
    }
  ]
}